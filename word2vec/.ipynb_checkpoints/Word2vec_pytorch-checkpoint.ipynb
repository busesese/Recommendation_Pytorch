{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/wenyi/Desktop/个人/学习/常用算法'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenyi/软件/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath('..')\n",
    "rating_path = os.path.join(path, 'data/ml-1m/ratings.dat')\n",
    "rating = pd.read_csv(rating_path, sep='::', names=['user','item', 'ratings', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>ratings</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  ratings  timestamp\n",
       "0     1  1193        5  978300760\n",
       "1     1   661        3  978302109\n",
       "2     1   914        3  978301968\n",
       "3     1  3408        4  978300275\n",
       "4     1  2355        5  978824291"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip gram model of word2vec\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Init parameter\n",
    "        vocab_size: word numberv of vocabulary\n",
    "        embed_dim: Embedding dimension, typically from 50 to 500\n",
    "        \"\"\"\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # input embedding project input word to a vector\n",
    "        self.input_embed = nn.Embedding(vocab_size, embed_dim, sparse=True)\n",
    "        # output embedding project output word to a vector\n",
    "        self.output_embed = nn.Embedding(vocab_size, embed_dim, sparse=True)\n",
    "        \n",
    "    def init_embed(self):\n",
    "        \"\"\"\n",
    "        Init embedding weight\n",
    "        input_embed is a uniform distribution is [-0.5/vocab_size, 0.5/vocab_size]\n",
    "        \"\"\"\n",
    "        init_range = 0.5/self.vocab_size\n",
    "        self.input_embed.weight.data.uniform_(-init_range, init_range)\n",
    "        self.output_embed.weight.data.uniform_(-0,0)\n",
    "        \n",
    "    def forward(self, pos_center_word, pos_neighbor_word, neg_neighbor_word):\n",
    "        \"\"\"\n",
    "        pos_center_word: list of center word id [1,3,5,...]\n",
    "        pos_neighbor_word: list of neighbor word id [3,5,6,...]\n",
    "        neg_neighbor_word: list[list] of negative neigbor word by negative sample the inner list is every\n",
    "        pos_neighbor_word sample n negative neighbor word [[2,4,5,7,8],[1,2,3,4,6],[1,2,3,4,5],...]\n",
    "        \"\"\"\n",
    "        # input word(center word) embeding\n",
    "        pos_center_embed = self.input_embed(pos_center_word)\n",
    "        # caculate positive sample loss\n",
    "        pos_neighbor_embed = self.output_embed(pos_neighbor_word)\n",
    "        pos_score = torch.mul(pos_center_embed, pos_neighbor_embed)\n",
    "        pos_score = torch.sum(pos_score, dim=-1)\n",
    "        pos_score = F.logsigmoid(pos_score)\n",
    "        \n",
    "        # caculate negative sample loss\n",
    "        # neg_neighbor_embed is a three dimension matrix different to pos_neighbor_embed\n",
    "        neg_neighbor_embed = self.output_embed(neg_neighbor_word) \n",
    "        neg_score = torch.bmm(neg_neighbor_embed, pos_center_embed.unsqueeze(2)).squeeze()\n",
    "        neg_score = F.logsigmoid(-1*neg_score)\n",
    "        \n",
    "        # loss\n",
    "        loss = -1*(torch.sum(pos_score) + torch.sum(neg_score))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    \"\"\"\n",
    "    Word2vec model\n",
    "    \"\"\"\n",
    "    def __init__(self, data,batch_size=128 ,iters=10,learning_rate=0.01,embeding_dim=100,windows_size=5, min_count=5):\n",
    "        \"\"\"\n",
    "        data: list[list] train data word id list for sentence\n",
    "        embeding_dim: embedding dimension\n",
    "        windows size: int, windows number for search neighbor word of a given center word\n",
    "        min_count: int, filter word if word frequency less than min_count \n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embeding_dim = embeding_dim\n",
    "        self.windows_size = windows_size\n",
    "        self.min_count = min_count\n",
    "        self.iters = iters\n",
    "        self.word2id = dict()\n",
    "        self.id2word = dict()\n",
    "        self.wordfrequency = self.dataprocess(data)\n",
    "        self.vocab_size = len(self.wordfrequency)\n",
    "        self.sample_table = self.init_sample_table()\n",
    "        self.model = SkipGramModel(self.vocab_size+1, self.embeding_dim)\n",
    "        self.optim = optim.SGD(self.model.parameters(), self.learning_rate)\n",
    "    \n",
    "    def dataprocess(self, data):\n",
    "        \"\"\"\n",
    "        filter word frequency less than min count and than construct a new word frequency dict\n",
    "        note:\n",
    "            the new word frequency dict is use the new word2id index project and we must save the\n",
    "            word2id and id2word project dict\n",
    "        \"\"\"\n",
    "        # count the data word frequency\n",
    "        wordfrequency = dict()\n",
    "        # filter word and use the new index to construct the dict\n",
    "        word_frequency = dict()\n",
    "        \n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                if word not in wordfrequency:\n",
    "                    wordfrequency[word] = 1\n",
    "                else:\n",
    "                    wordfrequency[word] += 1\n",
    "                    \n",
    "        idx = 0\n",
    "        for word, val in wordfrequency.items():\n",
    "            if val < self.min_count:\n",
    "                continue\n",
    "            self.word2id[word] = val\n",
    "            self.id2word[idx] = word\n",
    "            word_frequency[idx] = val\n",
    "            idx += 1\n",
    "        return word_frequency\n",
    "    \n",
    "    def init_sample_table(self):\n",
    "        \"\"\"\n",
    "        init teh sample tabel for negative sample \n",
    "        note:\n",
    "            sample_table_size is 10e8 like the word2vec doc and sample ratio is also use the offical paper\n",
    "        \"\"\"\n",
    "        sample_table_size = 10e8\n",
    "        sample_table = []\n",
    "        pow_frequency = np.array(list(self.wordfrequency.values()))**0.75\n",
    "        ratio = pow_frequency/sum(pow_frequency)\n",
    "        count = np.round(ratio*sample_table_size)\n",
    "        for i, c in enumerate(count):\n",
    "            sample_table += [i] * int(c)\n",
    "        sample_tabel = np.array(sample_table)\n",
    "        return sample_table\n",
    "        \n",
    "    def generate_train_data(self):\n",
    "        \"\"\"\n",
    "        generate the train data like (center word, neighbor word) pairs\n",
    "        \"\"\"\n",
    "        train_data = []\n",
    "        for sentence in self.data:\n",
    "            for i, word_u in enumerate(sentence):\n",
    "                for j, word_v in enumerate(sentence[max(i-self.windows_size,0):i+self.windows_size]):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    train_data.append((word_u, word_v))\n",
    "        return train_data\n",
    "    \n",
    "    def negative_sample_batch(self):\n",
    "        \"\"\"\n",
    "        the negative sample without ignore the target word(may be the negative sample word is the same\n",
    "        to the neighbor word)\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.sample_table, size=(self.batch_size, 5)).tolist()\n",
    "    \n",
    "    def train_batch(self, train_data):\n",
    "        \"\"\"\n",
    "        a generator for generate the train batch like [(center_word, neighbor_word),(center_word, neighbor_word)]\n",
    "        \"\"\"\n",
    "        iters = len(train_data)//self.batch_size + 1\n",
    "        for i in range(iters-1):\n",
    "            start = i*self.batch_size\n",
    "            end = (i+1)*self.batch_size\n",
    "            yield train_data[start:end]\n",
    "    \n",
    "    def train(self):\n",
    "        for k in range(self.iters):\n",
    "            train_data = self.generate_train_data()\n",
    "            dataloader = self.train_batch(train_data)\n",
    "            for i, data in enumerate(dataloader):\n",
    "                pos_centor_word = Variable(torch.LongTensor([pair[0] for pair in data]))\n",
    "                pos_neighbor_word = Variable(torch.LongTensor([pair[1] for pair in data]))\n",
    "                neg_neighbor_word = Variable(torch.LongTensor(self.negative_sample_batch()))\n",
    "                self.optim.zero_grad()\n",
    "                loss = self.model(pos_centor_word, pos_neighbor_word, neg_neighbor_word)\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "            print(\"Epoch %d is finished\" %(k+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 测试数据\n",
    "data = np.random.randint(1,50,size=(1000,8)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 is finished\n",
      "Epoch 2 is finished\n",
      "Epoch 3 is finished\n",
      "Epoch 4 is finished\n",
      "Epoch 5 is finished\n",
      "Epoch 6 is finished\n",
      "Epoch 7 is finished\n",
      "Epoch 8 is finished\n",
      "Epoch 9 is finished\n",
      "Epoch 10 is finished\n",
      "CPU times: user 1min 19s, sys: 690 ms, total: 1min 19s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2vec.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.7634, -0.5300,  0.3788,  ..., -0.0918,  0.1975, -0.7149],\n",
       "        [ 0.6912, -0.1294,  0.2411,  ...,  0.6027,  0.2645,  0.0246],\n",
       "        [-0.0079,  0.6770,  0.3081,  ...,  0.1047, -0.3630, -0.7708],\n",
       "        ...,\n",
       "        [ 1.0373,  0.1653,  0.4157,  ...,  0.5860, -0.7653, -0.2151],\n",
       "        [-0.1928,  0.0902,  0.5079,  ...,  0.5305,  1.3487,  0.0293],\n",
       "        [-0.4178,  0.5648,  1.0892,  ...,  0.5870, -0.8406, -0.6159]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the vecotr of every word after training \n",
    "word2vec.model.input_embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip gram model of word2vec\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Init parameter\n",
    "        vocab_size: word numberv of vocabulary\n",
    "        embed_dim: Embedding dimension, typically from 50 to 500\n",
    "        \"\"\"\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # input embedding project input word to a vector\n",
    "        self.input_embed = nn.Embedding(vocab_size, embed_dim, sparse=True)\n",
    "        # output embedding project output word to a vector\n",
    "        self.output_embed = nn.Embedding(vocab_size, embed_dim, sparse=True)\n",
    "        \n",
    "    def init_embed(self):\n",
    "        \"\"\"\n",
    "        Init embedding weight\n",
    "        input_embed is a uniform distribution is [-0.5/vocab_size, 0.5/vocab_size]\n",
    "        \"\"\"\n",
    "        init_range = 0.5/self.vocab_size\n",
    "        self.input_embed.weight.data.uniform_(-init_range, init_range)\n",
    "        self.output_embed.weight.data.uniform_(-0,0)\n",
    "        \n",
    "    def forward(self, pos_center_word, pos_neighbor_word, neg_neighbor_word):\n",
    "        \"\"\"\n",
    "        pos_center_word: list of center word id [1,3,5,...]\n",
    "        pos_neighbor_word: list of neighbor word id [3,5,6,...]\n",
    "        neg_neighbor_word: list[list] of negative neigbor word by negative sample the inner list is every\n",
    "        pos_neighbor_word sample n negative neighbor word [[2,4,5,7,8],[1,2,3,4,6],[1,2,3,4,5],...]\n",
    "        \"\"\"\n",
    "        # input word(center word) embeding\n",
    "        pos_center_embed = self.input_embed(pos_center_word)\n",
    "        # caculate positive sample loss\n",
    "        pos_neighbor_embed = self.output_embed(pos_neighbor_word)\n",
    "        pos_score = torch.mul(pos_center_embed, pos_neighbor_embed)\n",
    "        pos_score = torch.sum(pos_score, dim=-1)\n",
    "        pos_score = F.logsigmoid(pos_score)\n",
    "        \n",
    "        # caculate negative sample loss\n",
    "        # neg_neighbor_embed is a three dimension matrix different to pos_neighbor_embed\n",
    "        neg_neighbor_embed = self.output_embed(neg_neighbor_word) \n",
    "        neg_score = torch.bmm(neg_neighbor_embed, pos_center_embed.unsqueeze(2)).squeeze()\n",
    "        neg_score = F.logsigmoid(-1*neg_score)\n",
    "        \n",
    "        # loss\n",
    "        loss = -1*(torch.sum(pos_score) + torch.sum(neg_score))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    \"\"\"\n",
    "    Word2vec model\n",
    "    \"\"\"\n",
    "    def __init__(self, data,batch_size=128 ,iters=10,learning_rate=0.01,embeding_dim=100,windows_size=5, min_count=5):\n",
    "        \"\"\"\n",
    "        data: list[list] train data word id list for sentence\n",
    "        embeding_dim: embedding dimension\n",
    "        windows size: int, windows number for search neighbor word of a given center word\n",
    "        min_count: int, filter word if word frequency less than min_count \n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embeding_dim = embeding_dim\n",
    "        self.windows_size = windows_size\n",
    "        self.min_count = min_count\n",
    "        self.iters = iters\n",
    "        self.word2id = dict()\n",
    "        self.id2word = dict()\n",
    "        self.wordfrequency = self.dataprocess(data)\n",
    "        self.vocab_size = len(self.wordfrequency)\n",
    "        self.sample_table = self.init_sample_table()\n",
    "        self.model = SkipGramModel(self.vocab_size+1, self.embeding_dim)\n",
    "        self.optim = optim.SGD(self.model.parameters(), self.learning_rate)\n",
    "    \n",
    "    def dataprocess(self, data):\n",
    "        \"\"\"\n",
    "        filter word frequency less than min count and than construct a new word frequency dict\n",
    "        note:\n",
    "            the new word frequency dict is use the new word2id index project and we must save the\n",
    "            word2id and id2word project dict\n",
    "        \"\"\"\n",
    "        # count the data word frequency\n",
    "        wordfrequency = dict()\n",
    "        # filter word and use the new index to construct the dict\n",
    "        word_frequency = dict()\n",
    "        \n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                if word not in wordfrequency:\n",
    "                    wordfrequency[word] = 1\n",
    "                else:\n",
    "                    wordfrequency[word] += 1\n",
    "                    \n",
    "        idx = 0\n",
    "        for word, val in wordfrequency.items():\n",
    "            if val < self.min_count:\n",
    "                continue\n",
    "            self.word2id[word] = val\n",
    "            self.id2word[idx] = word\n",
    "            word_frequency[idx] = val\n",
    "            idx += 1\n",
    "        return word_frequency\n",
    "    \n",
    "    def init_sample_table(self):\n",
    "        \"\"\"\n",
    "        init teh sample tabel for negative sample \n",
    "        note:\n",
    "            sample_table_size is 10e8 like the word2vec doc and sample ratio is also use the offical paper\n",
    "        \"\"\"\n",
    "        sample_table_size = 10e4\n",
    "        sample_table = []\n",
    "        pow_frequency = np.array(list(self.wordfrequency.values()))**0.75\n",
    "        ratio = pow_frequency/sum(pow_frequency)\n",
    "        count = np.round(ratio*sample_table_size)\n",
    "        for i, c in enumerate(count):\n",
    "            sample_table += [i] * int(c)\n",
    "        sample_tabel = np.array(sample_table)\n",
    "        return sample_table\n",
    "        \n",
    "    def generate_train_data(self):\n",
    "        \"\"\"\n",
    "        generate the train data like (center word, neighbor word) pairs\n",
    "        \"\"\"\n",
    "        train_data = []\n",
    "        for sentence in self.data:\n",
    "            for i, word_u in enumerate(sentence):\n",
    "                for j, word_v in enumerate(sentence[max(i-self.windows_size,0):i+self.windows_size]):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    train_data.append((word_u, word_v))\n",
    "        return train_data\n",
    "    \n",
    "    def negative_sample_batch(self):\n",
    "        \"\"\"\n",
    "        the negative sample without ignore the target word(may be the negative sample word is the same\n",
    "        to the neighbor word)\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.sample_table, size=(self.batch_size, 5)).tolist()\n",
    "    \n",
    "    def train_batch(self, train_data):\n",
    "        \"\"\"\n",
    "        a generator for generate the train batch like [(center_word, neighbor_word),(center_word, neighbor_word)]\n",
    "        \"\"\"\n",
    "        iters = len(train_data)//self.batch_size + 1\n",
    "        for i in range(iters-1):\n",
    "            start = i*self.batch_size\n",
    "            end = (i+1)*self.batch_size\n",
    "            yield train_data[start:end]\n",
    "    \n",
    "    def train(self):\n",
    "        for k in range(self.iters):\n",
    "            train_data = self.generate_train_data()\n",
    "            dataloader = self.train_batch(train_data)\n",
    "            for i, data in enumerate(dataloader):\n",
    "                pos_centor_word = Variable(torch.LongTensor([pair[0] for pair in data]))\n",
    "                pos_neighbor_word = Variable(torch.LongTensor([pair[1] for pair in data]))\n",
    "                neg_neighbor_word = Variable(torch.LongTensor(self.negative_sample_batch()))\n",
    "                self.optim.zero_grad()\n",
    "                loss = self.model(pos_centor_word, pos_neighbor_word, neg_neighbor_word)\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "            print(\"Epoch %d is finished\" %(k+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 测试数据\n",
    "data = np.random.randint(1,50,size=(1000,8)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 is finished\n",
      "Epoch 2 is finished\n",
      "Epoch 3 is finished\n",
      "Epoch 4 is finished\n",
      "Epoch 5 is finished\n",
      "Epoch 6 is finished\n",
      "Epoch 7 is finished\n",
      "Epoch 8 is finished\n",
      "Epoch 9 is finished\n",
      "Epoch 10 is finished\n",
      "CPU times: user 1min 17s, sys: 1.54 s, total: 1min 18s\n",
      "Wall time: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2vec.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-2.4881e-02,  8.8162e-01,  6.3242e-02,  ...,  1.2183e+00,\n",
       "          3.6513e-01, -1.3111e+00],\n",
       "        [-3.6192e-01, -1.1458e-01, -6.4307e-02,  ...,  6.8155e-01,\n",
       "          2.1675e-02,  1.3356e-01],\n",
       "        [ 5.7432e-01,  5.0174e-01, -3.0563e-01,  ..., -1.7531e-01,\n",
       "          1.1257e+00, -1.0213e-01],\n",
       "        ...,\n",
       "        [ 3.6956e-01,  6.7997e-01, -3.2813e-01,  ..., -5.2111e-01,\n",
       "          1.1278e-01, -1.3773e-02],\n",
       "        [ 1.0463e-03, -2.5979e-01, -3.9221e-02,  ...,  1.4790e-02,\n",
       "         -5.0223e-02,  2.0891e-01],\n",
       "        [-3.9398e-01,  7.6720e-01,  3.1329e-01,  ...,  3.8423e-01,\n",
       "          4.4567e-01,  4.7976e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the vecotr of every word after training \n",
    "word2vec.model.input_embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

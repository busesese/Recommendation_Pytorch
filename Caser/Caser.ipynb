{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "batch_size = 512\n",
    "epochs = 20\n",
    "L = 5\n",
    "target_len = 3\n",
    "neg_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "path_train = os.path.join(os.path.abspath('.'), 'datasets','ml1m/test/train.txt')\n",
    "path_test = os.path.join(os.path.abspath('.'), 'datasets','ml1m/test/test.txt')\n",
    "train = pd.read_csv(path_train, sep=' ', names=['user', 'item', 'rating'])\n",
    "test = pd.read_csv(path_test, sep=' ', names=['user', 'item', 'rating'])\n",
    "\n",
    "# map user and item uuid to id\n",
    "user2id = {}\n",
    "item2id = {}\n",
    "for idx,user in enumerate(train['user'].unique().tolist()):\n",
    "    user2id[user] = idx\n",
    "for idx,item in enumerate(train['item'].unique().tolist()):\n",
    "    item2id[item] = idx\n",
    "train['user'] = train['user'].map(user2id)\n",
    "train['item'] = train['item'].map(item2id)\n",
    "test['user'] = test['user'].map(user2id)\n",
    "test['item'] = test['item'].map(item2id)\n",
    "\n",
    "# 将用户数据切分\n",
    "train_data = []\n",
    "test_data = []\n",
    "test_target = {}\n",
    "user_rating = {}\n",
    "for uid in train['user'].unique().tolist():\n",
    "    train_squence = train[train['user']==uid]['item'].tolist()\n",
    "    user_rating[uid] = train_squence\n",
    "    target = test[test['user']==uid]['item'].tolist()\n",
    "    test_target[uid] = target\n",
    "    for j in range(len(train_squence)-L-target_len):\n",
    "        item_sq = train_squence[j:j+L]\n",
    "        target_sq = train_squence[j+L:j+L+target_len]\n",
    "        train_data.append([uid]+item_sq+target_sq)\n",
    "    test_data.append([uid]+train_squence[-L:])\n",
    "\n",
    "# 构造训练Dataloader\n",
    "train_data = torch.LongTensor(train_data)\n",
    "train_x = train_data[:,:1+L]\n",
    "train_y = train_data[:,1+L:]\n",
    "test_data = torch.LongTensor(test_data)\n",
    "trainDataset = TensorDataset(train_x, train_y)\n",
    "dataLoader = DataLoader(trainDataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neg_sample(batch_size,userid, user_rating, neg_num, num_item):\n",
    "    neg_sample = []\n",
    "    for uid in userid:\n",
    "        result = []\n",
    "        while len(result)<neg_num:\n",
    "            neg_sampel_id = np.random.randint(0,num_item)\n",
    "            while neg_sampel_id in user_rating[uid]:\n",
    "                neg_sampel_id = np.random.randint(0,num_item)\n",
    "            result.append(neg_sampel_id)\n",
    "        neg_sample.append(result)\n",
    "    return torch.LongTensor(neg_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Caser(nn.Module):\n",
    "    def __init__(self, L, target_len, num_user,num_item,batch_size,outputs_h=16, outputs_v=4,embed_dimension=50):\n",
    "        super(Caser, self).__init__()\n",
    "        self.L = L\n",
    "        self.target_len = target_len\n",
    "        self.embed_dimension = embed_dimension\n",
    "        self.batch_size = batch_size\n",
    "        self.outputs_h = outputs_h\n",
    "        self.outputs_v = outputs_v\n",
    "        self.num_item = num_item\n",
    "        # embedding matrix\n",
    "        self.user_embedding = nn.Embedding(num_user, embed_dimension)\n",
    "        self.item_embedding = nn.Embedding(num_item, embed_dimension)\n",
    "\n",
    "        # horizontal convolution and pooling\n",
    "        self.conv_pools = nn.ModuleList(nn.Sequential(\n",
    "            nn.Conv2d(1, outputs_h, kernel_size=(k, embed_dimension)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(L-k+1, 1))) for k in range(1,L+1))\n",
    "\n",
    "        # vertical convolution\n",
    "        self.vertical_conv = nn.Conv2d(1, outputs_v, kernel_size=(L, 1))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # full conect layer\n",
    "        self.linear1 = nn.Linear(L*outputs_h+embed_dimension*outputs_v, embed_dimension)\n",
    "        \n",
    "        # full conect layer user item\n",
    "        self.weights = nn.Embedding(num_item,embed_dimension+embed_dimension)\n",
    "        self.bias = nn.Embedding(num_item,1)\n",
    "        \n",
    "        # embedding init\n",
    "        self.user_embedding.weight.data.normal_(0,1.0/self.user_embedding.embedding_dim)\n",
    "        self.item_embedding.weight.data.normal_(0,1.0/self.item_embedding.embedding_dim)\n",
    "        self.weights.weight.data.normal_(0,1.0/self.weights.embedding_dim)\n",
    "        self.bias.weight.data.zero_()\n",
    "        \n",
    "    def forward(self, user, items, y=None, for_pred=False):\n",
    "        user_embed = self.user_embedding(user)\n",
    "        user_embed = user_embed.squeeze(1)\n",
    "        item_embed = self.item_embedding(items)\n",
    "        \n",
    "        item_embed = item_embed.unsqueeze(1)\n",
    "        # horizontal convolution and pooling\n",
    "        item_x_h = [conv_pool(item_embed) for conv_pool in self.conv_pools]\n",
    "        item_x_h = torch.cat(item_x_h, 1)\n",
    "        # vertical convolution\n",
    "        item_x_v = self.vertical_conv(item_embed)\n",
    "        # concatenate item sequence vector\n",
    "        item_x_h = item_x_h.view(-1,self.outputs_h*self.L)\n",
    "        item_x_v = item_x_v.view(-1,self.outputs_v*self.embed_dimension)\n",
    "\n",
    "        x_c = torch.cat([item_x_h, item_x_v], 1)\n",
    "        x_c = self.dropout(x_c)\n",
    "        x_c = F.relu(self.linear1(x_c))\n",
    "        # concatenate user and item vector\n",
    "        x_u_i = torch.cat([x_c, user_embed], 1)\n",
    "        \n",
    "        if y is None:\n",
    "            y = torch.LongTensor(np.arange(self.num_item)).cuda()\n",
    "\n",
    "        weight = self.weights(y)\n",
    "        bias = self.bias(y)\n",
    "            \n",
    "        if for_pred == False:\n",
    "            # projection target target_len + negative_len\n",
    "            res = torch.baddbmm(bias, weight, x_u_i.unsqueeze(2)).squeeze()\n",
    "        else:\n",
    "            res = torch.mm(weight,torch.transpose(x_u_i,1,0)).squeeze()\n",
    "        return res       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_prec_recall(predict, target, k):\n",
    "    pred = predict[:k]\n",
    "    hits = len(set(pred).intersection(set(target)))\n",
    "    precision = hits/len(pred)\n",
    "    recall = hits/len(target)\n",
    "    return precision, recall\n",
    "\n",
    "def metric_map(predict, target, k):\n",
    "    if len(predict)>k:\n",
    "        predict = predict[:k]\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, val in enumerate(predict):\n",
    "        if val in target and val not in predict[:i]:\n",
    "            hits += 1\n",
    "            score += hits/(i+1.0)\n",
    "    if not list(target):\n",
    "        return 0.0\n",
    "    return score/min(len(target), k)\n",
    "\n",
    "def test(model, test_data, k):\n",
    "    model.eval()\n",
    "    prec = []\n",
    "    recall = []\n",
    "    maps = []\n",
    "    \n",
    "    userids = test_data[:,0].unsqueeze(1).cuda()\n",
    "    item_squence = test_data[:,1:].cuda()\n",
    "    for i in range(userids.size(0)):\n",
    "        uid = userids[i,:].unsqueeze(1)\n",
    "        predicts = model(uid, item_squence[i].unsqueeze(0), for_pred=True)\n",
    "        predicts = torch.argsort(predicts, descending=True)[:k].cpu().numpy()\n",
    "        target = test_target[i]\n",
    "        precision, rec = metric_prec_recall(predicts, target, k)\n",
    "        ap = metric_map(predicts, target, k)\n",
    "        prec.append(precision)\n",
    "        recall.append(rec)\n",
    "        maps.append(ap)\n",
    "    return np.mean(prec), np.mean(recall), np.mean(maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data, test_data, batch_size, neg_num, num_item,user_rating, epochs, lr=0.003):\n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay=0.000001, lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        losses = []\n",
    "        for idx, (train_x, train_y) in enumerate(dataLoader):\n",
    "            userid = train_x[:,0]\n",
    "            item_squence = train_x[:,1:].cuda()\n",
    "            target_squence = train_y\n",
    "            negative_sample = generate_neg_sample(batch_size, userid.numpy(), user_rating, neg_num, num_item)\n",
    "            y = torch.cat([target_squence, negative_sample],1).cuda()\n",
    "            userid = userid.unsqueeze(1).cuda()\n",
    "            y_pred = model(userid, item_squence, y)\n",
    "            y_pred_pos, y_pred_neg = torch.split(y_pred, [target_squence.size(1),negative_sample.size(1)], dim=1)\n",
    "            postive_loss = -torch.mean(torch.log(F.sigmoid(y_pred_pos)))\n",
    "            negative_loss = -torch.mean(torch.log(1- F.sigmoid(y_pred_neg)))\n",
    "            optimizer.zero_grad()\n",
    "            loss = postive_loss + negative_loss\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Epoch %d loss is %.3f and consume time is %.2f\" %(epoch+1, np.mean(losses), (time.time()-start)))\n",
    "        precision, recall, maps = test(model, test_data, k=10)\n",
    "        print(\"Test precision %.3f recall %.3f map %.3f\" %(precision, recall, maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss is 0.714 and consume time is 42.40\n",
      "Test precision 0.076 recall 0.052 map 0.032\n",
      "Epoch 2 loss is 0.465 and consume time is 40.92\n",
      "Test precision 0.090 recall 0.064 map 0.040\n",
      "Epoch 3 loss is 0.377 and consume time is 42.40\n",
      "Test precision 0.101 recall 0.076 map 0.048\n",
      "Epoch 4 loss is 0.332 and consume time is 44.26\n",
      "Test precision 0.103 recall 0.078 map 0.047\n",
      "Epoch 5 loss is 0.304 and consume time is 43.90\n",
      "Test precision 0.102 recall 0.076 map 0.047\n",
      "Epoch 6 loss is 0.287 and consume time is 43.92\n",
      "Test precision 0.105 recall 0.078 map 0.050\n",
      "Epoch 7 loss is 0.276 and consume time is 42.13\n",
      "Test precision 0.101 recall 0.075 map 0.047\n",
      "Epoch 8 loss is 0.267 and consume time is 41.65\n",
      "Test precision 0.101 recall 0.076 map 0.048\n",
      "Epoch 9 loss is 0.260 and consume time is 41.82\n",
      "Test precision 0.101 recall 0.076 map 0.047\n",
      "Epoch 10 loss is 0.255 and consume time is 42.94\n",
      "Test precision 0.102 recall 0.075 map 0.047\n",
      "Epoch 11 loss is 0.250 and consume time is 42.81\n",
      "Test precision 0.102 recall 0.076 map 0.048\n",
      "Epoch 12 loss is 0.247 and consume time is 42.12\n",
      "Test precision 0.102 recall 0.076 map 0.048\n",
      "Epoch 13 loss is 0.244 and consume time is 42.43\n",
      "Test precision 0.104 recall 0.077 map 0.050\n",
      "Epoch 14 loss is 0.242 and consume time is 42.69\n",
      "Test precision 0.105 recall 0.078 map 0.050\n",
      "Epoch 15 loss is 0.240 and consume time is 41.73\n",
      "Test precision 0.102 recall 0.076 map 0.048\n",
      "Epoch 16 loss is 0.238 and consume time is 40.92\n",
      "Test precision 0.101 recall 0.075 map 0.048\n",
      "Epoch 17 loss is 0.237 and consume time is 41.38\n",
      "Test precision 0.102 recall 0.077 map 0.048\n",
      "Epoch 18 loss is 0.235 and consume time is 41.39\n",
      "Test precision 0.100 recall 0.076 map 0.047\n",
      "Epoch 19 loss is 0.233 and consume time is 41.14\n",
      "Test precision 0.103 recall 0.077 map 0.049\n",
      "Epoch 20 loss is 0.233 and consume time is 41.75\n",
      "Test precision 0.102 recall 0.076 map 0.048\n"
     ]
    }
   ],
   "source": [
    "num_user = len(train['user'].unique())\n",
    "num_item = len(train['item'].unique())\n",
    "caser = Caser(L, target_len, num_user,num_item,batch_size).cuda()\n",
    "train_model(caser, dataLoader,test_data, batch_size, neg_num, num_item,user_rating, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
